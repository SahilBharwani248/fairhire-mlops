{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model A: AutoML for Recruiter Decision Classification\n",
        "\n",
        "Trains Model A using AutoML (PyCaret) to predict **Recruiter Decision (Hire/Reject)** based on resume features.\n",
        "\n",
        "## Model A Specifications:\n",
        "- **Input Features**: All resume features (Skills, Experience, Education, Certifications, Job Role, Salary, Projects)\n",
        "- **Excluded**: Demographic features (Gender, Race, Age, Disability_Status) - NOT used in training\n",
        "- **Target Variable**: Recruiter_Decision (Hire/Reject) - **Classification Task**\n",
        "- **Purpose**: This model acts as the company's hiring model we are testing for fairness\n",
        "\n",
        "## What we'll do:\n",
        "1. Load processed data from Data_processing.ipynb\n",
        "2. Set up PyCaret AutoML for classification\n",
        "3. Train and compare multiple models\n",
        "4. Evaluate metrics (Accuracy, Precision, Recall, F1, AUC)\n",
        "5. Save the best model and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n",
            "PyCaret version: 3.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyCaret for AutoML\n",
        "from pycaret.classification import *\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"PyCaret version: {__import__('pycaret').__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading processed data from CSV files...\n",
            "(Make sure Data_processing.ipynb has been run and saved all files)\n",
            "‚úì Loaded AI_Score files for fairness metrics\n",
            "‚úì Loaded demographics files for fairness metrics\n",
            "\n",
            "‚úì Successfully loaded all processed data from CSV files!\n",
            "\n",
            "Training set shape: (800, 7)\n",
            "Test set shape: (200, 7)\n",
            "\n",
            "Feature columns: ['Skills', 'Experience', 'Education_Ordinal', 'Certifications_Encoded', 'Job_Role_Encoded', 'Salary_Expectation', 'Projects_Count']\n",
            "\n",
            "Target variable (Recruiter_Decision) distribution:\n",
            "Recruiter_Decision\n",
            "Hire      650\n",
            "Reject    150\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class balance: {'Hire': 0.8125, 'Reject': 0.1875}\n"
          ]
        }
      ],
      "source": [
        "# Load processed data from saved CSV files\n",
        "print(\"Loading processed data from CSV files...\")\n",
        "print(\"(Make sure Data_processing.ipynb has been run and saved all files)\")\n",
        "\n",
        "try:\n",
        "    # Load training and test features\n",
        "    X_train = pd.read_csv('X_train.csv')\n",
        "    X_test = pd.read_csv('X_test.csv')\n",
        "    \n",
        "    # Load target variables (Recruiter_Decision) \n",
        "    y_train_df = pd.read_csv('y_train.csv')\n",
        "    y_test_df = pd.read_csv('y_test.csv')\n",
        "    y_train = y_train_df['Recruiter_Decision'].squeeze()  \n",
        "    y_test = y_test_df['Recruiter_Decision'].squeeze()   \n",
        "    \n",
        "    try:\n",
        "        ai_score_train = pd.read_csv('ai_score_train.csv')['AI_Score']  \n",
        "        ai_score_test = pd.read_csv('ai_score_test.csv')['AI_Score']    \n",
        "        print(\"‚úì Loaded AI_Score files for fairness metrics\")\n",
        "    except FileNotFoundError:\n",
        "        ai_score_train = None\n",
        "        ai_score_test = None\n",
        "        print(\"‚ö† AI_Score files not found (optional for fairness analysis)\")\n",
        "    \n",
        "    # Load demographics for fairness metrics (optional, for later use)\n",
        "    try:\n",
        "        demographics_train = pd.read_csv('demographics_train.csv')\n",
        "        demographics_test = pd.read_csv('demographics_test.csv')\n",
        "        print(\"‚úì Loaded demographics files for fairness metrics\")\n",
        "    except FileNotFoundError:\n",
        "        demographics_train = None\n",
        "        demographics_test = None\n",
        "        print(\"‚ö† Demographics files not found (optional for fairness analysis)\")\n",
        "    \n",
        "    print(\"\\n‚úì Successfully loaded all processed data from CSV files!\")\n",
        "    print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "    print(f\"Test set shape: {X_test.shape}\")\n",
        "    print(f\"\\nFeature columns: {list(X_train.columns)}\")\n",
        "    print(f\"\\nTarget variable (Recruiter_Decision) distribution:\")  \n",
        "    print(y_train.value_counts())  # CHANGED\n",
        "    print(f\"\\nClass balance: {y_train.value_counts(normalize=True).to_dict()}\")  \n",
        "    \n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n Error: Required CSV files not found!\")\n",
        "    print(f\"Missing file: {e.filename if hasattr(e, 'filename') else str(e)}\")\n",
        "    print(\"\\nPlease run Data_processing.ipynb first to generate the processed CSV files.\")\n",
        "    print(\"Required files:\")\n",
        "    print(\"  - X_train.csv, X_test.csv\")\n",
        "    print(\"  - y_train.csv, y_test.csv\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"\\n Error loading data: {e}\")\n",
        "    print(f\"Error type: {type(e).__name__}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data shape: (800, 8)\n",
            "Test data shape: (200, 8)\n",
            "\n",
            "Target distribution in training data:\n",
            "Recruiter_Decision\n",
            "Hire      650\n",
            "Reject    150\n",
            "Name: count, dtype: int64\n",
            "\n",
            "First few rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Skills</th>\n",
              "      <th>Experience</th>\n",
              "      <th>Education_Ordinal</th>\n",
              "      <th>Certifications_Encoded</th>\n",
              "      <th>Job_Role_Encoded</th>\n",
              "      <th>Salary_Expectation</th>\n",
              "      <th>Projects_Count</th>\n",
              "      <th>Recruiter_Decision</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Deep Learning, Python</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>91723</td>\n",
              "      <td>8</td>\n",
              "      <td>Hire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Linux, Ethical Hacking, Cybersecurity</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>96836</td>\n",
              "      <td>1</td>\n",
              "      <td>Hire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pytorch, NLP, TensorFlow, Python</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>51478</td>\n",
              "      <td>9</td>\n",
              "      <td>Hire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pytorch, NLP, Python, TensorFlow</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>94795</td>\n",
              "      <td>7</td>\n",
              "      <td>Hire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pytorch, Python</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>89338</td>\n",
              "      <td>1</td>\n",
              "      <td>Hire</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Skills  Experience  Education_Ordinal  \\\n",
              "0                  Deep Learning, Python           9                  2   \n",
              "1  Linux, Ethical Hacking, Cybersecurity           4                  4   \n",
              "2       Pytorch, NLP, TensorFlow, Python           7                  3   \n",
              "3       Pytorch, NLP, Python, TensorFlow           1                  3   \n",
              "4                        Pytorch, Python           5                  2   \n",
              "\n",
              "   Certifications_Encoded  Job_Role_Encoded  Salary_Expectation  \\\n",
              "0                       2                 2               91723   \n",
              "1                       0                 1               96836   \n",
              "2                       0                 0               51478   \n",
              "3                       2                 0               94795   \n",
              "4                       1                 0               89338   \n",
              "\n",
              "   Projects_Count Recruiter_Decision  \n",
              "0               8               Hire  \n",
              "1               1               Hire  \n",
              "2               9               Hire  \n",
              "3               7               Hire  \n",
              "4               1               Hire  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Combine X and y for PyCaret setup\n",
        "train_data = X_train.copy()\n",
        "train_data['Recruiter_Decision'] = y_train.values  \n",
        "\n",
        "test_data = X_test.copy()\n",
        "test_data['Recruiter_Decision'] = y_test.values   \n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Test data shape: {test_data.shape}\")\n",
        "print(f\"\\nTarget distribution in training data:\") \n",
        "print(train_data['Recruiter_Decision'].value_counts()) \n",
        "print(f\"\\nFirst few rows:\")\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Setup PyCaret AutoML\n",
        "\n",
        "Initialize PyCaret regression environment. This will:\n",
        "- Handle text features (Skills) automatically\n",
        "- Prepare data for modeling\n",
        "- Set up preprocessing pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e20a3_row10_col1, #T_e20a3_row16_col1, #T_e20a3_row18_col1, #T_e20a3_row20_col1, #T_e20a3_row22_col1 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e20a3\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e20a3_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
              "      <th id=\"T_e20a3_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_e20a3_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
              "      <td id=\"T_e20a3_row0_col1\" class=\"data row0 col1\" >42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_e20a3_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
              "      <td id=\"T_e20a3_row1_col1\" class=\"data row1 col1\" >Recruiter_Decision</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_e20a3_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
              "      <td id=\"T_e20a3_row2_col1\" class=\"data row2 col1\" >Binary</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_e20a3_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
              "      <td id=\"T_e20a3_row3_col1\" class=\"data row3 col1\" >Hire: 0, Reject: 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_e20a3_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
              "      <td id=\"T_e20a3_row4_col1\" class=\"data row4 col1\" >(1000, 8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_e20a3_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
              "      <td id=\"T_e20a3_row5_col1\" class=\"data row5 col1\" >(1000, 2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_e20a3_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
              "      <td id=\"T_e20a3_row6_col1\" class=\"data row6 col1\" >(800, 2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_e20a3_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
              "      <td id=\"T_e20a3_row7_col1\" class=\"data row7 col1\" >(200, 2)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_e20a3_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
              "      <td id=\"T_e20a3_row8_col1\" class=\"data row8 col1\" >6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_e20a3_row9_col0\" class=\"data row9 col0\" >Categorical features</td>\n",
              "      <td id=\"T_e20a3_row9_col1\" class=\"data row9 col1\" >1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_e20a3_row10_col0\" class=\"data row10 col0\" >Preprocess</td>\n",
              "      <td id=\"T_e20a3_row10_col1\" class=\"data row10 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_e20a3_row11_col0\" class=\"data row11 col0\" >Imputation type</td>\n",
              "      <td id=\"T_e20a3_row11_col1\" class=\"data row11 col1\" >simple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_e20a3_row12_col0\" class=\"data row12 col0\" >Numeric imputation</td>\n",
              "      <td id=\"T_e20a3_row12_col1\" class=\"data row12 col1\" >mean</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_e20a3_row13_col0\" class=\"data row13 col0\" >Categorical imputation</td>\n",
              "      <td id=\"T_e20a3_row13_col1\" class=\"data row13 col1\" >mode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_e20a3_row14_col0\" class=\"data row14 col0\" >Maximum one-hot encoding</td>\n",
              "      <td id=\"T_e20a3_row14_col1\" class=\"data row14 col1\" >25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_e20a3_row15_col0\" class=\"data row15 col0\" >Encoding method</td>\n",
              "      <td id=\"T_e20a3_row15_col1\" class=\"data row15 col1\" >None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_e20a3_row16_col0\" class=\"data row16 col0\" >Remove multicollinearity</td>\n",
              "      <td id=\"T_e20a3_row16_col1\" class=\"data row16 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_e20a3_row17_col0\" class=\"data row17 col0\" >Multicollinearity threshold</td>\n",
              "      <td id=\"T_e20a3_row17_col1\" class=\"data row17 col1\" >0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_e20a3_row18_col0\" class=\"data row18 col0\" >Transformation</td>\n",
              "      <td id=\"T_e20a3_row18_col1\" class=\"data row18 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_e20a3_row19_col0\" class=\"data row19 col0\" >Transformation method</td>\n",
              "      <td id=\"T_e20a3_row19_col1\" class=\"data row19 col1\" >yeo-johnson</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_e20a3_row20_col0\" class=\"data row20 col0\" >Normalize</td>\n",
              "      <td id=\"T_e20a3_row20_col1\" class=\"data row20 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_e20a3_row21_col0\" class=\"data row21 col0\" >Normalize method</td>\n",
              "      <td id=\"T_e20a3_row21_col1\" class=\"data row21 col1\" >zscore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_e20a3_row22_col0\" class=\"data row22 col0\" >Feature selection</td>\n",
              "      <td id=\"T_e20a3_row22_col1\" class=\"data row22 col1\" >True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_e20a3_row23_col0\" class=\"data row23 col0\" >Feature selection method</td>\n",
              "      <td id=\"T_e20a3_row23_col1\" class=\"data row23 col1\" >classic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_e20a3_row24_col0\" class=\"data row24 col0\" >Feature selection estimator</td>\n",
              "      <td id=\"T_e20a3_row24_col1\" class=\"data row24 col1\" >lightgbm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_e20a3_row25_col0\" class=\"data row25 col0\" >Number of features selected</td>\n",
              "      <td id=\"T_e20a3_row25_col1\" class=\"data row25 col1\" >0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_e20a3_row26_col0\" class=\"data row26 col0\" >Fold Generator</td>\n",
              "      <td id=\"T_e20a3_row26_col1\" class=\"data row26 col1\" >StratifiedKFold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_e20a3_row27_col0\" class=\"data row27 col0\" >Fold Number</td>\n",
              "      <td id=\"T_e20a3_row27_col1\" class=\"data row27 col1\" >10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_e20a3_row28_col0\" class=\"data row28 col0\" >CPU Jobs</td>\n",
              "      <td id=\"T_e20a3_row28_col1\" class=\"data row28 col1\" >-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_e20a3_row29_col0\" class=\"data row29 col0\" >Use GPU</td>\n",
              "      <td id=\"T_e20a3_row29_col1\" class=\"data row29 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_e20a3_row30_col0\" class=\"data row30 col0\" >Log Experiment</td>\n",
              "      <td id=\"T_e20a3_row30_col1\" class=\"data row30 col1\" >False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_e20a3_row31_col0\" class=\"data row31 col0\" >Experiment Name</td>\n",
              "      <td id=\"T_e20a3_row31_col1\" class=\"data row31 col1\" >clf-default-name</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e20a3_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_e20a3_row32_col0\" class=\"data row32 col0\" >USI</td>\n",
              "      <td id=\"T_e20a3_row32_col1\" class=\"data row32 col1\" >7bb0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1622a1060>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì PyCaret setup completed!\n",
            "\n",
            "Training samples: 800\n",
            "Test samples: 200\n"
          ]
        }
      ],
      "source": [
        "# Initialize PyCaret CLASSIFICATION environment \n",
        "clf = setup(  \n",
        "    data=train_data,\n",
        "    target='Recruiter_Decision',  \n",
        "    test_data=test_data,\n",
        "    session_id=42,\n",
        "    normalize=True,\n",
        "    transformation=True,\n",
        "    feature_selection=True,\n",
        "    remove_multicollinearity=True,\n",
        "    multicollinearity_threshold=0.95,\n",
        "    remove_outliers=False,\n",
        "    fix_imbalance=False,\n",
        "    verbose=True,\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"‚úì PyCaret setup completed!\")\n",
        "print(f\"\\nTraining samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compare Models\n",
        "\n",
        "Compare multiple regression models to find the best one for predicting AI Score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available classification models in PyCaret:\n",
            "                                     Name  \\\n",
            "ID                                          \n",
            "lr                    Logistic Regression   \n",
            "knn                K Neighbors Classifier   \n",
            "nb                            Naive Bayes   \n",
            "dt               Decision Tree Classifier   \n",
            "svm                   SVM - Linear Kernel   \n",
            "rbfsvm                SVM - Radial Kernel   \n",
            "gpc           Gaussian Process Classifier   \n",
            "mlp                        MLP Classifier   \n",
            "ridge                    Ridge Classifier   \n",
            "rf               Random Forest Classifier   \n",
            "qda       Quadratic Discriminant Analysis   \n",
            "ada                  Ada Boost Classifier   \n",
            "gbc          Gradient Boosting Classifier   \n",
            "lda          Linear Discriminant Analysis   \n",
            "et                 Extra Trees Classifier   \n",
            "lightgbm  Light Gradient Boosting Machine   \n",
            "dummy                    Dummy Classifier   \n",
            "\n",
            "                                                  Reference  Turbo  \n",
            "ID                                                                  \n",
            "lr        sklearn.linear_model._logistic.LogisticRegression   True  \n",
            "knn       sklearn.neighbors._classification.KNeighborsCl...   True  \n",
            "nb                           sklearn.naive_bayes.GaussianNB   True  \n",
            "dt             sklearn.tree._classes.DecisionTreeClassifier   True  \n",
            "svm       sklearn.linear_model._stochastic_gradient.SGDC...   True  \n",
            "rbfsvm                             sklearn.svm._classes.SVC  False  \n",
            "gpc       sklearn.gaussian_process._gpc.GaussianProcessC...  False  \n",
            "mlp       sklearn.neural_network._multilayer_perceptron....  False  \n",
            "ridge           sklearn.linear_model._ridge.RidgeClassifier   True  \n",
            "rf          sklearn.ensemble._forest.RandomForestClassifier   True  \n",
            "qda       sklearn.discriminant_analysis.QuadraticDiscrim...   True  \n",
            "ada       sklearn.ensemble._weight_boosting.AdaBoostClas...   True  \n",
            "gbc         sklearn.ensemble._gb.GradientBoostingClassifier   True  \n",
            "lda       sklearn.discriminant_analysis.LinearDiscrimina...   True  \n",
            "et            sklearn.ensemble._forest.ExtraTreesClassifier   True  \n",
            "lightgbm                    lightgbm.sklearn.LGBMClassifier   True  \n",
            "dummy                         sklearn.dummy.DummyClassifier   True  \n"
          ]
        }
      ],
      "source": [
        "# Check available classification models\n",
        "from pycaret.classification import models\n",
        "\n",
        "available_models = models()\n",
        "print(\"Available classification models in PyCaret:\")\n",
        "print(available_models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model comparison completed!\n",
            "\n",
            "Top 5 models selected based on AUC\n"
          ]
        }
      ],
      "source": [
        "# Compare CLASSIFICATION models and select top performers \n",
        "best_models = compare_models(\n",
        "    include=['rf','lightgbm', 'et', 'ada', 'dt', 'lr', 'ridge', 'nb', 'knn'], \n",
        "    sort='AUC', \n",
        "    n_select=5,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"‚úì Model comparison completed!\")\n",
        "print(f\"\\nTop 5 models selected based on AUC\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model: RandomForestClassifier\n",
            "\n",
            "Model performance on test set:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4e11f1bbdf9416895e58b9f7e4fdbfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get the best model (lowest RMSE)\n",
        "best_model = best_models[0] if isinstance(best_models, list) else best_models\n",
        "\n",
        "print(f\"Best model: {type(best_model).__name__}\")\n",
        "print(\"\\nModel performance on test set:\")\n",
        "evaluate_model(best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Model Tuning (Optional)\n",
        "\n",
        "Fine-tune the best model to improve performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model tuning completed!\n",
            "\n",
            "Tuned model: RandomForestClassifier\n",
            "\n",
            "Tuned model performance:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f06c83caab641efa5a586d8db6bd576",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Tune the best model\n",
        "tuned_model = tune_model(best_model, optimize='AUC', n_iter=50, verbose=False)\n",
        "\n",
        "print(\"‚úì Model tuning completed!\")\n",
        "print(f\"\\nTuned model: {type(tuned_model).__name__}\")\n",
        "print(\"\\nTuned model performance:\")\n",
        "evaluate_model(tuned_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Final Model Selection\n",
        "\n",
        "Select the final model (tuned or original) and evaluate on test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_5abe8\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_5abe8_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_5abe8_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_5abe8_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
              "      <th id=\"T_5abe8_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_5abe8_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
              "      <th id=\"T_5abe8_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_5abe8_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
              "      <th id=\"T_5abe8_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_5abe8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_5abe8_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
              "      <td id=\"T_5abe8_row0_col1\" class=\"data row0 col1\" >0.6950</td>\n",
              "      <td id=\"T_5abe8_row0_col2\" class=\"data row0 col2\" >0.4974</td>\n",
              "      <td id=\"T_5abe8_row0_col3\" class=\"data row0 col3\" >0.6950</td>\n",
              "      <td id=\"T_5abe8_row0_col4\" class=\"data row0 col4\" >0.6980</td>\n",
              "      <td id=\"T_5abe8_row0_col5\" class=\"data row0 col5\" >0.6965</td>\n",
              "      <td id=\"T_5abe8_row0_col6\" class=\"data row0 col6\" >0.0190</td>\n",
              "      <td id=\"T_5abe8_row0_col7\" class=\"data row0 col7\" >0.0190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x162636860>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FINAL MODEL PERFORMANCE METRICS\n",
            "============================================================\n",
            "\n",
            "Model Type: RandomForestClassifier\n",
            "\n",
            "Classification Metrics (Recruiter Decision Prediction):\n",
            "  Accuracy:  0.6950\n",
            "  Precision: 0.8137\n",
            "  Recall:    0.8086\n",
            "  F1 Score:  0.8111\n",
            "  AUC-ROC:   0.5306\n",
            "\n",
            "Confusion Matrix:\n",
            "[[131  31]\n",
            " [ 30   8]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Hire       0.81      0.81      0.81       162\n",
            "      Reject       0.21      0.21      0.21        38\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.51      0.51      0.51       200\n",
            "weighted avg       0.70      0.69      0.70       200\n",
            "\n",
            "\n",
            "Test Set Size: 200 samples\n",
            "Class Distribution: {'Hire': 162, 'Reject': 38}\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Use tuned model if it performs better, otherwise use original\n",
        "#in our case tuned_model performed worse, so we consider best_model\n",
        "final_model = best_model\n",
        "\n",
        "# Make predictions on test set\n",
        "predictions = predict_model(final_model, data=test_data)\n",
        "\n",
        "# Calculate CLASSIFICATION metrics \n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "y_true = test_data['Recruiter_Decision'].values  \n",
        "y_pred = predictions['prediction_label'].values\n",
        "y_pred_proba = predictions['prediction_score'].values if 'prediction_score' in predictions.columns else None\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, pos_label='Hire')  \n",
        "recall = recall_score(y_true, y_pred, pos_label='Hire')       \n",
        "f1 = f1_score(y_true, y_pred, pos_label='Hire')               \n",
        "\n",
        "# AUC requires probability scores\n",
        "if y_pred_proba is not None:\n",
        "    # Convert labels to binary (Hire=1, Reject=0)\n",
        "    y_true_binary = (y_true == 'Hire').astype(int)\n",
        "    auc = roc_auc_score(y_true_binary, y_pred_proba)\n",
        "else:\n",
        "    auc = None\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nModel Type: {type(final_model).__name__}\")\n",
        "print(f\"\\nClassification Metrics (Recruiter Decision Prediction):\")  \n",
        "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"  Precision: {precision:.4f}\")\n",
        "print(f\"  Recall:    {recall:.4f}\")\n",
        "print(f\"  F1 Score:  {f1:.4f}\")\n",
        "if auc is not None:\n",
        "    print(f\"  AUC-ROC:   {auc:.4f}\")\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "print(f\"\\nTest Set Size: {len(y_true)} samples\")\n",
        "print(f\"Class Distribution: {pd.Series(y_true).value_counts().to_dict()}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Metrics Summary:\n",
            "  model_type: RandomForestClassifier\n",
            "  accuracy: 0.695\n",
            "  precision: 0.8136645962732919\n",
            "  recall: 0.808641975308642\n",
            "  f1_score: 0.8111455108359134\n",
            "  auc: 0.5306205328135153\n",
            "  test_samples: 200\n",
            "  target_variable: Recruiter_Decision\n",
            "  task_type: classification\n",
            "  positive_class: Hire\n",
            "  class_distribution: {'Hire': 162, 'Reject': 38}\n"
          ]
        }
      ],
      "source": [
        "# Store metrics in a dictionary\n",
        "metrics = {\n",
        "    'model_type': type(final_model).__name__,\n",
        "    'accuracy': float(accuracy),\n",
        "    'precision': float(precision),\n",
        "    'recall': float(recall),\n",
        "    'f1_score': float(f1),\n",
        "    'auc': float(auc) if auc is not None else None,\n",
        "    'test_samples': int(len(y_true)),\n",
        "    'target_variable': 'Recruiter_Decision',  \n",
        "    'task_type': 'classification',  \n",
        "    'positive_class': 'Hire',  \n",
        "    'class_distribution': pd.Series(y_true).value_counts().to_dict()  \n",
        "}\n",
        "\n",
        "# Display metrics\n",
        "print(\"\\nMetrics Summary:\")\n",
        "for key, value in metrics.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our baseline hiring model (Random Forest) achieves respectable precision and recall (~0.81), indicating reasonable predictive ability. However, the low AUC (0.53) reflects limited discrimination between Hire and Reject classes because the dataset is highly imbalanced (162 Hire vs. 38 Reject). This is realistic for HR scenarios, where models often overpredict hires due to skewed training data. This imperfect model is ideal for our FairHire system, which aims to evaluate fairness, detect bias, and monitor drift‚Äîrather than optimize predictive performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================================\n",
            "Building Model A shortlist base for all candidates\n",
            "===================================================\n",
            "Loading Model A...\n",
            "Transformation Pipeline and Model Successfully Loaded\n",
            "‚úì Model A loaded successfully\n",
            "‚úì Loaded Dataset_A_processed.csv with shape: (1000, 13)\n",
            "‚úì All 7 feature columns present\n",
            "Making predictions on all candidates...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_184d3\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_184d3_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_184d3_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_184d3_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
              "      <th id=\"T_184d3_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_184d3_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
              "      <th id=\"T_184d3_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_184d3_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
              "      <th id=\"T_184d3_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_184d3_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_184d3_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
              "      <td id=\"T_184d3_row0_col1\" class=\"data row0 col1\" >0.9380</td>\n",
              "      <td id=\"T_184d3_row0_col2\" class=\"data row0 col2\" >0.8975</td>\n",
              "      <td id=\"T_184d3_row0_col3\" class=\"data row0 col3\" >0.9380</td>\n",
              "      <td id=\"T_184d3_row0_col4\" class=\"data row0 col4\" >0.9380</td>\n",
              "      <td id=\"T_184d3_row0_col5\" class=\"data row0 col5\" >0.9380</td>\n",
              "      <td id=\"T_184d3_row0_col6\" class=\"data row0 col6\" >0.7969</td>\n",
              "      <td id=\"T_184d3_row0_col7\" class=\"data row0 col7\" >0.7969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x16403ffa0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Predictions complete:\n",
            "  - Predicted Hire: 812 (81.2%)\n",
            "  - Predicted Reject: 188 (18.8%)\n",
            "\n",
            "‚úì Shortlist base created:\n",
            "  - Total candidates: 1000\n",
            "  - Rank range: 1 to 999\n",
            "  - Top candidate hire probability: 1.000\n",
            "  - Bottom candidate hire probability: 0.540\n",
            "\n",
            "‚úì Saved: model_A/modelA_shortlist_base.csv\n",
            "Columns: ['Skills', 'Experience', 'Education', 'Certifications', 'Job_Role', 'Recruiter_Decision', 'Salary_Expectation', 'Projects_Count', 'AI_Score', 'Education_Ordinal', 'Certifications_Encoded', 'Job_Role_Encoded', 'Education_Encoded', 'ModelA_Hire_Prob', 'ModelA_Prediction', 'ModelA_Pred_Label', 'ModelA_Rank']\n",
            "\n",
            "üèÜ Top 5 Candidates (Highest Hire Probability):\n",
            "     ModelA_Rank               Job_Role  Experience  Education_Ordinal  \\\n",
            "0              1          AI Researcher          10                  2   \n",
            "528            1         Data Scientist           6                  3   \n",
            "527            1  Cybersecurity Analyst           0                  2   \n",
            "524            1      Software Engineer           7                  2   \n",
            "516            1      Software Engineer           9                  3   \n",
            "\n",
            "     ModelA_Hire_Prob ModelA_Pred_Label  \n",
            "0                 1.0              Hire  \n",
            "528               1.0              Hire  \n",
            "527               1.0              Hire  \n",
            "524               1.0              Hire  \n",
            "516               1.0              Hire  \n",
            "\n",
            "üìâ Bottom 5 Candidates (Lowest Hire Probability):\n",
            "     ModelA_Rank               Job_Role  Experience  Education_Ordinal  \\\n",
            "753          996         Data Scientist           0                  2   \n",
            "559          997  Cybersecurity Analyst           1                  4   \n",
            "15           997  Cybersecurity Analyst           1                  4   \n",
            "673          999  Cybersecurity Analyst           0                  2   \n",
            "283          999          AI Researcher           3                  2   \n",
            "\n",
            "     ModelA_Hire_Prob ModelA_Pred_Label  \n",
            "753              0.56            Reject  \n",
            "559              0.55            Reject  \n",
            "15               0.55            Reject  \n",
            "673              0.54            Reject  \n",
            "283              0.54            Reject  \n",
            "\n",
            "===================================================\n",
            "‚úÖ Model A shortlist base complete!\n",
            "===================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pycaret.classification import load_model, predict_model\n",
        "\n",
        "print(\"\\n===================================================\")\n",
        "print(\"Building Model A shortlist base for all candidates\")\n",
        "print(\"===================================================\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load Model A using PyCaret's load_model (includes preprocessing)\n",
        "# ============================================================\n",
        "print(\"Loading Model A...\")\n",
        "modelA = load_model('model_A/modelA_final')  # ‚ö†Ô∏è Don't include .pkl extension!\n",
        "print(\"‚úì Model A loaded successfully\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Load full processed dataset\n",
        "# ============================================================\n",
        "full_df = pd.read_csv(\"model_A/Dataset_A_processed.csv\")\n",
        "print(f\"‚úì Loaded Dataset_A_processed.csv with shape: {full_df.shape}\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Prepare data for prediction (need target column for PyCaret)\n",
        "# ============================================================\n",
        "# PyCaret expects the target column to be present (even if we ignore it)\n",
        "# Create a dummy target column if it doesn't exist\n",
        "if 'Recruiter_Decision' not in full_df.columns:\n",
        "    full_df['Recruiter_Decision'] = 'Hire'  # Dummy value\n",
        "\n",
        "# Feature columns that were used in training\n",
        "feature_columns = [\n",
        "    'Skills', \n",
        "    'Experience', \n",
        "    'Education_Ordinal', \n",
        "    'Certifications_Encoded', \n",
        "    'Job_Role_Encoded', \n",
        "    'Salary_Expectation', \n",
        "    'Projects_Count'\n",
        "]\n",
        "\n",
        "# Sanity check: make sure all feature columns exist\n",
        "missing_feats = [c for c in feature_columns if c not in full_df.columns]\n",
        "if missing_feats:\n",
        "    print(f\"‚ö†Ô∏è Missing feature columns in Dataset_A_processed: {missing_feats}\")\n",
        "    raise ValueError(\"Feature columns mismatch between training and full dataset.\")\n",
        "\n",
        "print(f\"‚úì All {len(feature_columns)} feature columns present\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. Make predictions using PyCaret's predict_model\n",
        "# ============================================================\n",
        "print(\"Making predictions on all candidates...\")\n",
        "predictions = predict_model(modelA, data=full_df)\n",
        "\n",
        "# Extract predictions and probabilities\n",
        "pred_labels = predictions['prediction_label'].values\n",
        "pred_scores = predictions['prediction_score'].values  # This is P(Hire)\n",
        "\n",
        "# Convert labels to binary\n",
        "label_map = {'Hire': 1, 'Reject': 0}\n",
        "pred_binary = predictions['prediction_label'].map(label_map).fillna(0).astype(int).values\n",
        "\n",
        "print(f\"‚úì Predictions complete:\")\n",
        "print(f\"  - Predicted Hire: {(pred_binary == 1).sum()} ({(pred_binary == 1).mean()*100:.1f}%)\")\n",
        "print(f\"  - Predicted Reject: {(pred_binary == 0).sum()} ({(pred_binary == 0).mean()*100:.1f}%)\")\n",
        "\n",
        "# ============================================================\n",
        "# 5. Build shortlist base DataFrame with rankings\n",
        "# ============================================================\n",
        "shortlist_base = full_df.copy()\n",
        "\n",
        "# Add prediction columns\n",
        "shortlist_base[\"ModelA_Hire_Prob\"] = pred_scores\n",
        "shortlist_base[\"ModelA_Prediction\"] = pred_binary\n",
        "shortlist_base[\"ModelA_Pred_Label\"] = predictions['prediction_label'].values\n",
        "\n",
        "# Add ranking based on hire probability (1 = highest probability)\n",
        "shortlist_base[\"ModelA_Rank\"] = shortlist_base[\"ModelA_Hire_Prob\"].rank(\n",
        "    ascending=False,  # Higher probability = better rank\n",
        "    method='min'      # Ties get same rank\n",
        ").astype(int)\n",
        "\n",
        "# Sort by rank for easy viewing\n",
        "shortlist_base = shortlist_base.sort_values('ModelA_Rank')\n",
        "\n",
        "print(f\"\\n‚úì Shortlist base created:\")\n",
        "print(f\"  - Total candidates: {len(shortlist_base)}\")\n",
        "print(f\"  - Rank range: {shortlist_base['ModelA_Rank'].min()} to {shortlist_base['ModelA_Rank'].max()}\")\n",
        "print(f\"  - Top candidate hire probability: {shortlist_base['ModelA_Hire_Prob'].max():.3f}\")\n",
        "print(f\"  - Bottom candidate hire probability: {shortlist_base['ModelA_Hire_Prob'].min():.3f}\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Save to CSV for Streamlit app\n",
        "# ============================================================\n",
        "output_path = \"model_A/modelA_shortlist_base.csv\"\n",
        "shortlist_base.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úì Saved: {output_path}\")\n",
        "print(f\"Columns: {shortlist_base.columns.tolist()}\")\n",
        "\n",
        "# ============================================================\n",
        "# 7. Display sample of top and bottom candidates\n",
        "# ============================================================\n",
        "print(\"\\nüèÜ Top 5 Candidates (Highest Hire Probability):\")\n",
        "top_cols = ['ModelA_Rank', 'Job_Role', 'Experience', 'Education_Ordinal', \n",
        "            'ModelA_Hire_Prob', 'ModelA_Pred_Label']\n",
        "print(shortlist_base[top_cols].head())\n",
        "\n",
        "print(\"\\nüìâ Bottom 5 Candidates (Lowest Hire Probability):\")\n",
        "print(shortlist_base[top_cols].tail())\n",
        "\n",
        "print(\"\\n===================================================\")\n",
        "print(\"‚úÖ Model A shortlist base complete!\")\n",
        "print(\"===================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#For streamlit UI implementation for top rank candidates for a job role...\n",
        "\n",
        "# Original columns from Dataset_A_processed.csv\n",
        "'Skills', 'Experience', 'Education_Ordinal', 'Certifications_Encoded', \n",
        "'Job_Role_Encoded', 'Salary_Expectation', 'Projects_Count', 'Recruiter_Decision'\n",
        "\n",
        "# NEW columns added:\n",
        "'ModelA_Hire_Prob'   # Probability of being hired (0.0 to 1.0)\n",
        "'ModelA_Prediction'  # Binary prediction (1=Hire, 0=Reject)\n",
        "'ModelA_Pred_Label'  # Text label ('Hire' or 'Reject')\n",
        "'ModelA_Rank'        # Ranking (1 = best candidate)\n",
        "\n",
        "# In your Streamlit app\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "# Load shortlist\n",
        "shortlist = pd.read_csv('model_A/modelA_shortlist_base.csv')\n",
        "\n",
        "# Filter by job role\n",
        "job_role = st.selectbox(\"Select Job Role\", shortlist['Job_Role'].unique())\n",
        "filtered = shortlist[shortlist['Job_Role_Encoded'] == job_role]\n",
        "\n",
        "# Show top K candidates\n",
        "top_k = st.slider(\"Number of candidates to show\", 10, 100, 50)\n",
        "top_candidates = filtered.nsmallest(top_k, 'ModelA_Rank')\n",
        "\n",
        "# Display with ranking\n",
        "st.dataframe(top_candidates[['ModelA_Rank', 'Skills', 'Experience', \n",
        "                              'Education_Ordinal', 'ModelA_Hire_Prob']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save Model and Metrics\n",
        "\n",
        "Save the trained model and evaluation metrics for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformation Pipeline and Model Successfully Saved\n",
            "‚úì Model saved as 'modelA_final.pkl'\n",
            "  (PyCaret automatically saves the model with preprocessing pipeline)\n"
          ]
        }
      ],
      "source": [
        "# Save the final model\n",
        "save_model(final_model, 'modelA_final')\n",
        "\n",
        "print(\"‚úì Model saved as 'modelA_final.pkl'\")\n",
        "print(\"  (PyCaret automatically saves the model with preprocessing pipeline)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Metrics saved to 'modelA_metrics.json'\n",
            "\n",
            "Saved metrics:\n",
            "{\n",
            "  \"model_type\": \"RandomForestClassifier\",\n",
            "  \"accuracy\": 0.695,\n",
            "  \"precision\": 0.8136645962732919,\n",
            "  \"recall\": 0.808641975308642,\n",
            "  \"f1_score\": 0.8111455108359134,\n",
            "  \"auc\": 0.5306205328135153,\n",
            "  \"test_samples\": 200,\n",
            "  \"target_variable\": \"Recruiter_Decision\",\n",
            "  \"task_type\": \"classification\",\n",
            "  \"positive_class\": \"Hire\",\n",
            "  \"class_distribution\": {\n",
            "    \"Hire\": 162,\n",
            "    \"Reject\": 38\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Save metrics to JSON file\n",
        "with open('modelA_metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=4)\n",
        "\n",
        "print(\"‚úì Metrics saved to 'modelA_metrics.json'\")\n",
        "print(\"\\nSaved metrics:\")\n",
        "print(json.dumps(metrics, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Model Visualization\n",
        "\n",
        "Visualize model performance and feature importance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model plots saved in 'Plots' directory\n"
          ]
        }
      ],
      "source": [
        "plot_model(final_model, plot='auc', save=True)           # ROC-AUC curve\n",
        "plot_model(final_model, plot='confusion_matrix', save=True)  # Confusion matrix\n",
        "plot_model(final_model, plot='class_report', save=True)  # Classification report\n",
        "plot_model(final_model, plot='pr', save=True)            # Precision-Recall curve\n",
        "plot_model(final_model, plot='feature', save=True)       # Feature importance\n",
        "\n",
        "print(\"‚úì Model plots saved in 'Plots' directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Summary\n",
        "\n",
        "Model A training completed! This model can now be used for:\n",
        "- Predicting AI Score based on resume features\n",
        "- Fairness analysis (using demographics_test and y_class_test from Data_processing.ipynb)\n",
        "- Comparison with other models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MODEL A TRAINING SUMMARY\n",
            "============================================================\n",
            "\n",
            "‚úì Model Type: RandomForestClassifier\n",
            "‚úì Target Variable: Recruiter_Decision (Classification)\n",
            "‚úì Positive Class: Hire\n",
            "‚úì Test Set Performance:\n",
            "    Accuracy:  0.6950\n",
            "    Precision: 0.8137\n",
            "    Recall:    0.8086\n",
            "    F1 Score:  0.8111\n",
            "    AUC-ROC:   0.5306\n",
            "\n",
            "‚úì Files Saved:\n",
            "    - modelA_final.pkl (trained model)\n",
            "    - modelA_metrics.json (evaluation metrics)\n",
            "    - modelA_predictions.csv (test set predictions)\n",
            "\n",
            "‚úì Next Steps:\n",
            "    - Use this model for fairness analysis\n",
            "    - Compare predictions across demographic groups\n",
            "    - Analyze bias in Hire/Reject decisions\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"MODEL A TRAINING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n‚úì Model Type: {metrics['model_type']}\")\n",
        "print(f\"‚úì Target Variable: {metrics['target_variable']} (Classification)\")  \n",
        "print(f\"‚úì Positive Class: {metrics['positive_class']}\")  \n",
        "print(f\"‚úì Test Set Performance:\")\n",
        "print(f\"    Accuracy:  {metrics['accuracy']:.4f}\")  \n",
        "print(f\"    Precision: {metrics['precision']:.4f}\") \n",
        "print(f\"    Recall:    {metrics['recall']:.4f}\")     \n",
        "print(f\"    F1 Score:  {metrics['f1_score']:.4f}\")   \n",
        "if metrics['auc'] is not None:\n",
        "    print(f\"    AUC-ROC:   {metrics['auc']:.4f}\")     \n",
        "print(f\"\\n‚úì Files Saved:\")\n",
        "print(f\"    - modelA_final.pkl (trained model)\")\n",
        "print(f\"    - modelA_metrics.json (evaluation metrics)\")\n",
        "print(f\"    - modelA_predictions.csv (test set predictions)\")\n",
        "print(f\"\\n‚úì Next Steps:\")\n",
        "print(f\"    - Use this model for fairness analysis\")\n",
        "print(f\"    - Compare predictions across demographic groups\")  \n",
        "print(f\"    - Analyze bias in Hire/Reject decisions\")         \n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Fairness Analysis\n",
        "\n",
        "- Importing fairness_util.py which adheres to fairness metrics from arXiv paper: https://arxiv.org/pdf/2405.19699 \n",
        "(Fairness in AI-Driven Recruitment: Challenges, Metrics, Methods, and Future Directions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            " FAIRNESS ANALYSIS FOR MODEL A\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_ec376\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ec376_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
              "      <th id=\"T_ec376_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_ec376_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
              "      <th id=\"T_ec376_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
              "      <th id=\"T_ec376_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
              "      <th id=\"T_ec376_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
              "      <th id=\"T_ec376_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
              "      <th id=\"T_ec376_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ec376_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ec376_row0_col0\" class=\"data row0 col0\" >Random Forest Classifier</td>\n",
              "      <td id=\"T_ec376_row0_col1\" class=\"data row0 col1\" >0.6950</td>\n",
              "      <td id=\"T_ec376_row0_col2\" class=\"data row0 col2\" >0.4974</td>\n",
              "      <td id=\"T_ec376_row0_col3\" class=\"data row0 col3\" >0.6950</td>\n",
              "      <td id=\"T_ec376_row0_col4\" class=\"data row0 col4\" >0.6980</td>\n",
              "      <td id=\"T_ec376_row0_col5\" class=\"data row0 col5\" >0.6965</td>\n",
              "      <td id=\"T_ec376_row0_col6\" class=\"data row0 col6\" >0.0190</td>\n",
              "      <td id=\"T_ec376_row0_col7\" class=\"data row0 col7\" >0.0190</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x1621605e0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Fairness Analysis Setup:\n",
            "  - Test samples: 200\n",
            "  - Ground truth (Hire=1): 162 (81.0%)\n",
            "  - Model predicts (Hire=1): 161 (80.5%)\n",
            "  - Model accuracy: 69.5%\n",
            "\n",
            "‚úì Fairness DataFrame:\n",
            "  - Shape: (200, 7)\n",
            "  - Demographics: ['Gender', 'Race', 'Age_Group', 'Disability_Status']\n",
            "\n",
            "======================================================================\n",
            "üìä Fairness Analysis by: Gender\n",
            "======================================================================\n",
            "\n",
            "Group distribution (kept groups):\n",
            "group\n",
            "Male      107\n",
            "Female     93\n",
            "Name: count, dtype: int64\n",
            "\n",
            "1Ô∏è‚É£ Demographic Parity (Hire Rate by Group):\n",
            "   Female: 83.9%\n",
            "   Male: 77.6%\n",
            "   ‚ö†Ô∏è Max gap: 6.3 percentage points (lower = fairer)\n",
            "\n",
            "2Ô∏è‚É£ Top-50 Selection Rate Parity:\n",
            "   Female: 24.7% in Top-50\n",
            "   Male: 25.2% in Top-50\n",
            "   Min/Max ratio: 0.980\n",
            "    PASS (4/5 rule: ‚â• 0.80)\n",
            "\n",
            "3Ô∏è‚É£ Equal Opportunity (TPR among true Hires):\n",
            "   Female: 100.0%\n",
            "   Male: 100.0%\n",
            "    Max TPR gap: 0.0 percentage points\n",
            "\n",
            "4Ô∏è‚É£ Rank Ordering Bias (lower avg rank = appears earlier in shortlist):\n",
            "   Female: average rank 102.7\n",
            "   Male: average rank 98.6\n",
            "    Max rank gap: 4.2 positions\n",
            "\n",
            "======================================================================\n",
            "üìä Fairness Analysis by: Race\n",
            "======================================================================\n",
            "\n",
            "Group distribution (kept groups):\n",
            "group\n",
            "White       111\n",
            "Hispanic     33\n",
            "Black        28\n",
            "Asian        22\n",
            "Name: count, dtype: int64\n",
            "\n",
            "1Ô∏è‚É£ Demographic Parity (Hire Rate by Group):\n",
            "   Asian: 81.8%\n",
            "   Black: 82.1%\n",
            "   Hispanic: 66.7%\n",
            "   White: 83.8%\n",
            "   ‚ö†Ô∏è Max gap: 17.1 percentage points (lower = fairer)\n",
            "\n",
            "2Ô∏è‚É£ Top-50 Selection Rate Parity:\n",
            "   Asian: 31.8% in Top-50\n",
            "   Black: 25.0% in Top-50\n",
            "   Hispanic: 18.2% in Top-50\n",
            "   White: 27.0% in Top-50\n",
            "   Min/Max ratio: 0.571\n",
            "    FAIL (4/5 rule: < 0.80)\n",
            "\n",
            "3Ô∏è‚É£ Equal Opportunity (TPR among true Hires):\n",
            "   Asian: 100.0%\n",
            "   Black: 100.0%\n",
            "   Hispanic: 100.0%\n",
            "   White: 100.0%\n",
            "    Max TPR gap: 0.0 percentage points\n",
            "\n",
            "4Ô∏è‚É£ Rank Ordering Bias (lower avg rank = appears earlier in shortlist):\n",
            "   Asian: average rank 94.4\n",
            "   Black: average rank 98.2\n",
            "   Hispanic: average rank 109.1\n",
            "   White: average rank 94.5\n",
            "    Max rank gap: 14.7 positions\n",
            "\n",
            "======================================================================\n",
            "üìä Fairness Analysis by: Age_Group\n",
            "======================================================================\n",
            "\n",
            "Group distribution (kept groups):\n",
            "group\n",
            "18-29    112\n",
            "30-39     86\n",
            "40+        0\n",
            "Name: count, dtype: int64\n",
            "\n",
            "1Ô∏è‚É£ Demographic Parity (Hire Rate by Group):\n",
            "   18-29: 83.0%\n",
            "   30-39: 76.7%\n",
            "   40+: nan%\n",
            "   ‚ö†Ô∏è Max gap: nan percentage points (lower = fairer)\n",
            "\n",
            "2Ô∏è‚É£ Top-50 Selection Rate Parity:\n",
            "   18-29: 27.7% in Top-50\n",
            "   30-39: 22.1% in Top-50\n",
            "   40+: nan% in Top-50\n",
            "   Min/Max ratio: 0.000\n",
            "    FAIL (4/5 rule: < 0.80)\n",
            "\n",
            "3Ô∏è‚É£ Equal Opportunity (TPR among true Hires):\n",
            "   18-29: 100.0%\n",
            "   30-39: 100.0%\n",
            "   40+: N/A (no true positives)\n",
            "    Max TPR gap: 0.0 percentage points\n",
            "\n",
            "4Ô∏è‚É£ Rank Ordering Bias (lower avg rank = appears earlier in shortlist):\n",
            "   18-29: average rank 98.7\n",
            "   30-39: average rank 100.6\n",
            "   40+: average rank nan\n",
            "    Max rank gap: nan positions\n",
            "\n",
            "======================================================================\n",
            "üìä Fairness Analysis by: Disability_Status\n",
            "======================================================================\n",
            "\n",
            "Group distribution (kept groups):\n",
            "group\n",
            "No     187\n",
            "Yes     13\n",
            "Name: count, dtype: int64\n",
            "\n",
            "1Ô∏è‚É£ Demographic Parity (Hire Rate by Group):\n",
            "   No: 80.7%\n",
            "   Yes: 76.9%\n",
            "   ‚ö†Ô∏è Max gap: 3.8 percentage points (lower = fairer)\n",
            "\n",
            "2Ô∏è‚É£ Top-50 Selection Rate Parity:\n",
            "   No: 25.1% in Top-50\n",
            "   Yes: 23.1% in Top-50\n",
            "   Min/Max ratio: 0.918\n",
            "    PASS (4/5 rule: ‚â• 0.80)\n",
            "\n",
            "3Ô∏è‚É£ Equal Opportunity (TPR among true Hires):\n",
            "   No: 100.0%\n",
            "   Yes: 100.0%\n",
            "    Max TPR gap: 0.0 percentage points\n",
            "\n",
            "4Ô∏è‚É£ Rank Ordering Bias (lower avg rank = appears earlier in shortlist):\n",
            "   No: average rank 100.4\n",
            "   Yes: average rank 102.2\n",
            "    Max rank gap: 1.9 positions\n",
            "\n",
            "======================================================================\n",
            "===  FAIRNESS SUMMARY FOR MODEL A (COMPACT) ===\n",
            "======================================================================\n",
            "\n",
            "üîç Gender:\n",
            "   demographic_parity_max_gap: 0.0630\n",
            "   topk_min_over_max: 0.9801\n",
            "   equal_opportunity_max_gap: 0.0000\n",
            "   rank_ordering_max_gap: 4.1503\n",
            "   score_distribution_overlap: 0.8615\n",
            "\n",
            "üîç Race:\n",
            "   demographic_parity_max_gap: 0.1712\n",
            "   topk_min_over_max: 0.5714\n",
            "   equal_opportunity_max_gap: 0.0000\n",
            "   rank_ordering_max_gap: 14.6818\n",
            "   score_distribution_overlap: N/A\n",
            "\n",
            "üîç Age_Group:\n",
            "   demographic_parity_max_gap: nan\n",
            "   topk_min_over_max: 0.0000\n",
            "   equal_opportunity_max_gap: 0.0000\n",
            "   rank_ordering_max_gap: nan\n",
            "   score_distribution_overlap: 0.7635\n",
            "\n",
            "üîç Disability_Status:\n",
            "   demographic_parity_max_gap: 0.0383\n",
            "   topk_min_over_max: 0.9182\n",
            "   equal_opportunity_max_gap: 0.0000\n",
            "   rank_ordering_max_gap: 1.8511\n",
            "   score_distribution_overlap: 0.5825\n",
            "\n",
            "‚úì Saved: modelA_fairness_metrics.json\n",
            "‚úì Updated: modelA_metrics.json\n",
            "\n",
            "======================================================================\n",
            " FAIRNESS ANALYSIS COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "from fairness_util import (\n",
        "    demographic_parity_diff,\n",
        "    selection_rate_parity_topk,\n",
        "    rank_ordering_bias,\n",
        "    equal_opportunity_diff,\n",
        "    score_distribution_overlap,\n",
        ")\n",
        "\n",
        "print(\"\\n======================================================================\")\n",
        "print(\" FAIRNESS ANALYSIS FOR MODEL A\")\n",
        "print(\"======================================================================\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1. Prepare ground truth, predictions, base fairness DataFrame\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Map Recruiter_Decision text -> binary 1/0\n",
        "label_map = {\"Hire\": 1, \"Reject\": 0, \"No hire\": 0, \"No Hire\": 0}\n",
        "\n",
        "# Get true labels from test_data\n",
        "y_true = test_data['Recruiter_Decision'].map(label_map).fillna(0).astype(int)\n",
        "\n",
        "# Get predictions from your PyCaret model\n",
        "predictions_full = predict_model(final_model, data=test_data)\n",
        "\n",
        "# Extract predicted labels\n",
        "y_pred_labels = predictions_full['prediction_label'].map(label_map).fillna(0).astype(int)\n",
        "y_pred = y_pred_labels.values\n",
        "\n",
        "# Extract probability of Hire class\n",
        "if 'prediction_score_Hire' in predictions_full.columns:\n",
        "    y_proba = predictions_full['prediction_score_Hire'].values\n",
        "elif 'prediction_score' in predictions_full.columns:\n",
        "    y_proba = predictions_full['prediction_score'].values\n",
        "else:\n",
        "    print(\" Warning: Using fallback probability extraction\")\n",
        "    y_proba = predict_model(final_model, data=test_data, raw_score=True)['prediction_score'].values\n",
        "\n",
        "accuracy = (y_pred == y_true).mean()\n",
        "\n",
        "print(\"‚úì Fairness Analysis Setup:\")\n",
        "print(f\"  - Test samples: {len(y_true)}\")\n",
        "print(f\"  - Ground truth (Hire=1): {y_true.sum()} ({y_true.mean()*100:.1f}%)\")\n",
        "print(f\"  - Model predicts (Hire=1): {y_pred.sum()} ({y_pred.mean()*100:.1f}%)\")\n",
        "print(f\"  - Model accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "fair_df = pd.DataFrame({\n",
        "    \"y_true\":  y_true.values,\n",
        "    \"y_pred\":  y_pred,\n",
        "    \"y_proba\": y_proba,\n",
        "})\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Clean demographics and attach to fairness DataFrame\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "# Load demographics if not already in memory\n",
        "if 'demographics_test' not in locals() or demographics_test is None:\n",
        "    try:\n",
        "        demographics_test = pd.read_csv('demographics_test.csv')\n",
        "        print(\"‚úì Loaded demographics_test.csv\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ö†Ô∏è Warning: demographics_test.csv not found!\")\n",
        "        demographics_test = pd.DataFrame()\n",
        "\n",
        "demo = demographics_test.copy()\n",
        "\n",
        "# Age -> bucket into coarse groups so output is readable\n",
        "if \"Age\" in demo.columns:\n",
        "    demo[\"Age_Group\"] = pd.cut(\n",
        "        demo[\"Age\"],\n",
        "        bins=[17, 29, 39, 120],\n",
        "        labels=[\"18-29\", \"30-39\", \"40+\"],\n",
        "        right=True,\n",
        "        include_lowest=True,\n",
        "    )\n",
        "\n",
        "# Race -> collapse rare categories into \"Other / Minority\"\n",
        "if \"Race\" in demo.columns:\n",
        "    race_counts = demo[\"Race\"].value_counts()\n",
        "    rare_races = race_counts[race_counts < 10].index  # threshold can be tuned\n",
        "    demo[\"Race_Grouped\"] = demo[\"Race\"].where(~demo[\"Race\"].isin(rare_races),\n",
        "                                              \"Other / Minority\")\n",
        "\n",
        "# Attach cleaned demographics using your original column names where possible\n",
        "if \"Gender\" in demo.columns:\n",
        "    fair_df[\"Gender\"] = demo[\"Gender\"].values\n",
        "if \"Race_Grouped\" in demo.columns:\n",
        "    fair_df[\"Race\"] = demo[\"Race_Grouped\"].values   # we overwrite Race with grouped\n",
        "elif \"Race\" in demo.columns:\n",
        "    fair_df[\"Race\"] = demo[\"Race\"].values\n",
        "if \"Age_Group\" in demo.columns:\n",
        "    fair_df[\"Age_Group\"] = demo[\"Age_Group\"].values\n",
        "if \"Disability_Status\" in demo.columns:\n",
        "    fair_df[\"Disability_Status\"] = demo[\"Disability_Status\"].values\n",
        "\n",
        "demographic_attrs = [\"Gender\", \"Race\", \"Age_Group\", \"Disability_Status\"]\n",
        "demographic_attrs = [a for a in demographic_attrs if a in fair_df.columns]\n",
        "\n",
        "print(\"\\n‚úì Fairness DataFrame:\")\n",
        "print(f\"  - Shape: {fair_df.shape}\")\n",
        "print(f\"  - Demographics: {demographic_attrs}\")\n",
        "\n",
        "# Only report groups with at least this many samples\n",
        "MIN_GROUP_SIZE = 10\n",
        "\n",
        "def filter_small_groups(df, group_col, min_size=MIN_GROUP_SIZE):\n",
        "    counts = df[group_col].value_counts()\n",
        "    valid = counts[counts >= min_size].index\n",
        "    return df[df[group_col].isin(valid)], valid\n",
        "\n",
        "\n",
        "fairness_summary_A = {}\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Per-attribute fairness metrics (clean, human-readable)\n",
        "# -------------------------------------------------------------------\n",
        "for attr in demographic_attrs:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üìä Fairness Analysis by: {attr}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    df_attr = fair_df.copy()\n",
        "    df_attr[\"group\"] = df_attr[attr]\n",
        "\n",
        "    # Filter out tiny groups to avoid crazy 100%/0% stats from 1‚Äì2 rows\n",
        "    df_attr, kept_groups = filter_small_groups(df_attr, \"group\", MIN_GROUP_SIZE)\n",
        "    if len(kept_groups) < 2:\n",
        "        print(f\"‚ö†Ô∏è Not enough data for {attr} after filtering groups with <{MIN_GROUP_SIZE} samples. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGroup distribution (kept groups):\")\n",
        "    print(df_attr[\"group\"].value_counts())\n",
        "\n",
        "    # 1Ô∏è‚É£ Demographic Parity ‚Äì use y_pred directly (hire rate by group)\n",
        "    dp = demographic_parity_diff(\n",
        "        df_attr,\n",
        "        group_col=\"group\",\n",
        "        score_col=\"y_pred\",  # mean of y_pred == hire rate\n",
        "        threshold=None,\n",
        "    )\n",
        "    print(\"\\n1Ô∏è‚É£ Demographic Parity (Hire Rate by Group):\")\n",
        "    for g, r in dp[\"per_group\"].items():\n",
        "        print(f\"   {g}: {r*100:.1f}%\")\n",
        "    print(f\"   ‚ö†Ô∏è Max gap: {dp['max_gap']*100:.1f} percentage points (lower = fairer)\")\n",
        "\n",
        "    # 2Ô∏è‚É£ Top-K Selection Rate Parity\n",
        "    top_k = min(50, len(df_attr))\n",
        "    srp = selection_rate_parity_topk(\n",
        "        df_attr,\n",
        "        group_col=\"group\",\n",
        "        score_col=\"y_proba\",\n",
        "        k=top_k,\n",
        "    )\n",
        "    print(f\"\\n2Ô∏è‚É£ Top-{top_k} Selection Rate Parity:\")\n",
        "    for g, r in srp[\"per_group\"].items():\n",
        "        print(f\"   {g}: {r*100:.1f}% in Top-{top_k}\")\n",
        "    print(f\"   Min/Max ratio: {srp['min_over_max']:.3f}\")\n",
        "    if srp[\"min_over_max\"] < 0.8:\n",
        "        print(\"    FAIL (4/5 rule: < 0.80)\")\n",
        "    else:\n",
        "        print(\"    PASS (4/5 rule: ‚â• 0.80)\")\n",
        "\n",
        "    # 3Ô∏è‚É£ Equal Opportunity ‚Äì TPR parity across groups\n",
        "    eo = equal_opportunity_diff(\n",
        "        df_attr,\n",
        "        group_col=\"group\",\n",
        "        y_true_col=\"y_true\",\n",
        "        score_col=\"y_proba\",\n",
        "        positive_label=1,\n",
        "        threshold=0.5,\n",
        "    )\n",
        "    print(\"\\n3Ô∏è‚É£ Equal Opportunity (TPR among true Hires):\")\n",
        "    for g, tpr in eo[\"per_group_tpr\"].items():\n",
        "        if np.isnan(tpr):\n",
        "            print(f\"   {g}: N/A (no true positives)\")\n",
        "        else:\n",
        "            print(f\"   {g}: {tpr*100:.1f}%\")\n",
        "    print(f\"    Max TPR gap: {eo['max_tpr_gap']*100:.1f} percentage points\")\n",
        "\n",
        "    # 4Ô∏è‚É£ Rank Ordering Bias\n",
        "    rob = rank_ordering_bias(\n",
        "        df_attr,\n",
        "        group_col=\"group\",\n",
        "        score_col=\"y_proba\",\n",
        "    )\n",
        "    print(\"\\n4Ô∏è‚É£ Rank Ordering Bias (lower avg rank = appears earlier in shortlist):\")\n",
        "    for g, r in rob[\"per_group_avg_rank\"].items():\n",
        "        print(f\"   {g}: average rank {r:.1f}\")\n",
        "    print(f\"    Max rank gap: {rob['max_rank_gap']:.1f} positions\")\n",
        "\n",
        "    # Optional score overlap if exactly 2 groups kept\n",
        "    kept_groups_list = list(kept_groups)\n",
        "    if len(kept_groups_list) == 2:\n",
        "        sdo = score_distribution_overlap(\n",
        "            df_attr,\n",
        "            group_a=kept_groups_list[0],\n",
        "            group_b=kept_groups_list[1],\n",
        "            group_col=\"group\",\n",
        "            score_col=\"y_proba\",\n",
        "            bins=20,\n",
        "        )\n",
        "    else:\n",
        "        sdo = None\n",
        "\n",
        "    fairness_summary_A[attr] = {\n",
        "        \"demographic_parity_max_gap\": float(dp[\"max_gap\"]),\n",
        "        \"topk_min_over_max\": float(srp[\"min_over_max\"]),\n",
        "        \"equal_opportunity_max_gap\": float(eo[\"max_tpr_gap\"]) if not np.isnan(eo[\"max_tpr_gap\"]) else None,\n",
        "        \"rank_ordering_max_gap\": float(rob[\"max_rank_gap\"]),\n",
        "        \"score_distribution_overlap\": float(sdo) if sdo is not None else None,\n",
        "    }\n",
        "\n",
        "print(\"\\n======================================================================\")\n",
        "print(\"===  FAIRNESS SUMMARY FOR MODEL A (COMPACT) ===\")\n",
        "print(\"======================================================================\")\n",
        "for attr, metrics_dict in fairness_summary_A.items():\n",
        "    print(f\"\\nüîç {attr}:\")\n",
        "    for k, v in metrics_dict.items():\n",
        "        if v is not None:\n",
        "            print(f\"   {k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"   {k}: N/A\")\n",
        "\n",
        "# Save summary to JSON\n",
        "with open(\"modelA_fairness_metrics.json\", \"w\") as f:\n",
        "    json.dump(fairness_summary_A, f, indent=2)\n",
        "print(\"\\n‚úì Saved: modelA_fairness_metrics.json\")\n",
        "\n",
        "# Update main metrics file\n",
        "try:\n",
        "    with open(\"modelA_metrics.json\", \"r\") as f:\n",
        "        modelA_metrics = json.load(f)\n",
        "except FileNotFoundError:\n",
        "    modelA_metrics = {}\n",
        "\n",
        "modelA_metrics[\"fairness\"] = fairness_summary_A\n",
        "\n",
        "with open(\"modelA_metrics.json\", \"w\") as f:\n",
        "    json.dump(modelA_metrics, f, indent=2)\n",
        "\n",
        "print(\"‚úì Updated: modelA_metrics.json\")\n",
        "\n",
        "print(\"\\n======================================================================\")\n",
        "print(\" FAIRNESS ANALYSIS COMPLETE!\")\n",
        "print(\"======================================================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Why Race Shows Bias:\n",
        "Looking at the numbers:\n",
        "\n",
        "Hispanic candidates: 66.7% hire rate, only 18.2% in top 50, ranked 109.1 on average\n",
        "Asian candidates: 81.8% hire rate, 31.8% in top 50, ranked 94.4 on average\n",
        "White candidates: 83.8% hire rate, 27.0% in top 50, ranked 94.5 on average\n",
        "\n",
        "This suggests that the model systematically ranks Hispanic candidates lower. Even when Hispanic candidates should be hired (ground truth), they appear lower in rankings. **This could violate equal opportunity laws (disparate impact)**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlopsproj",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
